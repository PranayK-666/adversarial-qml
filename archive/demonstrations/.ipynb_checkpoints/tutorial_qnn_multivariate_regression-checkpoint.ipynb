{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# This cell is added by sphinx-gallery\n# It can be customized to whatever you like\n%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Multidimensional regression with a variational quantum circuit\n===========================================================\n\nIn this tutorial, we show how to use a variational quantum circuit to\nfit the simple multivariate function\n\n$$f(x_1, x_2) = \\frac{1}{2} \\left( x_1^2 + x_2^2 \\right).$$\n\nIn it has been shown that, under some conditions, there exist\nvariational quantum circuits that are expressive enough to realize any\npossible set of Fourier coefficients. We will use a simple two-qubit\nparameterized quantum circuit to construct a partial Fourier series for\nfitting the target function.\n\nThe main outline of the process is as follows:\n\n1.  Build a circuit consisting of layers of alternating data-encoding\n    and parameterized training blocks.\n2.  Optimize the expectation value of the circuit output against a\n    target function to be fitted.\n3.  Obtain a partial Fourier series for the target function. Since the\n    function is not periodic, this partial Fourier series will only\n    approximate the function in the region we will use for training.\n4.  Plot the optimized circuit expectation value against the exact\n    function to compare the two.\n\n# What is a quantum model?\n\nA quantum model $g_{\\vec{\\theta}}(\\vec{x})$ is the expectation value of\nsome observable $M$ estimated on the state prepared by a parameterized\ncircuit $U(\\vec{x}, \\vec{\\theta})$:\n\n$$g_{\\vec{\\theta}}(\\vec{x}) = \\langle 0 | U^\\dagger (\\vec{x}, \\vec{\\theta}) M U(\\vec{x}, \\vec{\\theta}) | 0 \\rangle.$$\n\nBy repeatedly running the circuit with a set of parameters\n$\\vec{\\theta}$ and set of data points $\\vec{x}$, we can approximate the\nexpectation value of the observable $M$ in the state\n$U(\\vec{x}, \\vec{\\theta}) | 0 \\rangle.$ Then, the parameters can be\noptimized to minimize some loss function.\n\n# Building the variational circuit\n\nIn this example, we will use a variational quantum circuit to find the\nFourier series that approximates the function\n$f(x_1, x_2) = \\frac{1}{2} \\left( x_1^2 + x_2^2 \\right)$. The\nvariational circuit that we are using is made up of $L$ layers. Each\nlayer consists of a *data-encoding block* $S(\\vec{x})$ and a *training\nblock* $W(\\vec{\\theta})$. The overall circuit is:\n\n$$U(\\vec{x}, \\vec{\\theta}) = W^{(L+1)}(\\vec{\\theta}) S(\\vec{x}) W^{(L)} (\\vec{\\theta}) \\ldots W^{(2)}(\\vec{\\theta}) S(\\vec{x}) W^{(1)}(\\vec{\\theta}).$$\n\nThe training blocks $W(\\vec{\\theta})$ depend on the parameters\n$\\vec{\\theta}$ that can be optimized classically.\n\n![](../_static/demonstration_assets/qnn_multivariate_regression/qnn_circuit.png){.align-center\nwidth=\"90.0%\"}\n\nWe will build a circuit such that the expectation value of the\n$Z\\otimes Z$ observable is a partial Fourier series that approximates\n$f(\\vec{x})$, i.e.,\n\n$$g_{\\vec{\\theta}}(\\vec{x})= \\sum_{\\vec{\\omega} \\in \\Omega} c_\\vec{\\omega} e^{i \\vec{\\omega} \\vec{x}} \\approx f(\\vec{x}).$$\n\nThen, we can directly plot the partial Fourier series. We can also apply\na Fourier transform to $g_{\\vec{\\theta}}$, so we can obtain the Fourier\ncoefficients, $c_\\vec{\\omega}$. To know more about how to obtain the\nFourier series, check out these two related tutorials,.\n\n# Constructing the quantum circuit\n\nFirst, let\\'s import the necessary libraries and seed the random number\ngenerator. We will use Matplotlib for plotting and JAX for optimization.\nWe will also define the device, which has two qubits, using\n`~.pennylane.device`{.interpreted-text role=\"func\"}.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nimport pennylane as qml\nfrom pennylane import numpy as pnp\nimport jax\nfrom jax import numpy as jnp\nimport optax\n\npnp.random.seed(42)\n\ndev = qml.device('default.qubit', wires=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we will construct the data-encoding circuit block, $S(\\vec{x})$, as\na product of $R_z$ rotations:\n\n$$S(\\vec{x}) = R_z(x_1) \\otimes R_z(x_2).$$\n\nSpecifically, we define the $S(\\vec{x})$ operator using the\n`~.pennylane.AngleEmbedding`{.interpreted-text role=\"class\"} function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def S(x):\n    qml.AngleEmbedding( x, wires=[0,1],rotation='Z')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For the $W(\\vec{\\theta})$ operator, we will use an ansatz that is\navailable in PennyLane, called\n`~.pennylane.StronglyEntanglingLayers`{.interpreted-text role=\"class\"}.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def W(params):\n    qml.StronglyEntanglingLayers(params, wires=[0,1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we will build the circuit in PennyLane by alternating layers of\n$W(\\vec{\\theta})$ and $S(\\vec{x})$ layers. On this prepared state, we\nestimate the expectation value of the $Z\\otimes Z$ operator, using\nPennyLane\\'s `~.pennylane.expval`{.interpreted-text role=\"func\"}\nfunction.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "@qml.qnode(dev,interface=\"jax\")\ndef quantum_neural_network(params, x):\n    layers=len(params[:,0,0])-1\n    n_wires=len(params[0,:,0])\n    n_params_rot=len(params[0,0,:])\n    for i in range(layers):\n      W(params[i,:,:].reshape(1,n_wires,n_params_rot))\n      S(x)\n    W(params[-1,:,:].reshape(1,n_wires,n_params_rot))\n\n    return qml.expval(qml.PauliZ(wires=0)@qml.PauliZ(wires=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The function we will be fitting is\n$f(x_1, x_2) = \\frac{1}{2} \\left( x_1^2 + x_2^2 \\right)$, which we will\ndefine as `target_function`:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def target_function(x):\n    f=1/2*(x[0]**2+x[1]**2)\n    return f"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we will specify the range of $x_1$ and $x_2$ values and store those\nvalues in an input data vector. We are fitting the function for\n$x_1, x_2 \\in [-1, 1]$ using 30 evenly spaced samples for each variable.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "x1_min=-1\nx1_max=1\nx2_min=-1\nx2_max=1\nnum_samples=30"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we build the training data with the exact target function\n$f(x_1, x_2)$. To do so, it is convenient to create a two-dimensional\ngrid to make sure that, for each value of $x_1,$ we perform a sweep over\nall the values of $x_2$ and viceversa.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "x1_train=pnp.linspace(x1_min,x1_max, num_samples)\nx2_train=pnp.linspace(x2_min,x2_max, num_samples)\nx1_mesh,x2_mesh=pnp.meshgrid(x1_train, x2_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We define `x_train` and `y_train` using the above vectors, reshaping\nthem for our convenience\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "x_train=pnp.stack((x1_mesh.flatten(), x2_mesh.flatten()), axis=1)\ny_train = target_function([x1_mesh,x2_mesh]).reshape(-1,1)\n# Let's take a look at how they look like\nprint(\"x_train:\\n\", x_train[:5])\nprint(\"y_train:\\n\", y_train[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Optimizing the circuit\n\nWe want to optimize the circuit above so that the expectation value of\n$Z \\otimes Z$ approximates the exact target function. This is done by\nminimizing the mean squared error between the circuit output and the\nexact target function. In particular, the optimization process to train\nthe variational circuit will be performed using JAX, an auto\ndifferentiable machine learning framework to accelerate the classical\noptimization of the parameters. Check out to learn more about how to use\nJAX to optimize your QML models.\n\n![](../_static/demonstration_assets/qnn_multivariate_regression/qnn_diagram.jpg){.align-center\nwidth=\"90.0%\"}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "@jax.jit\ndef mse(params,x,targets):\n    # We compute the mean square error between the target function and the quantum circuit to quantify the quality of our estimator\n    return (quantum_neural_network(params,x)-jnp.array(targets))**2\n@jax.jit\ndef loss_fn(params, x,targets):\n    # We define the loss function to feed our optimizer\n    mse_pred = jax.vmap(mse,in_axes=(None, 0,0))(params,x,targets)\n    loss = jnp.mean(mse_pred)\n    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here, we are choosing an Adam optimizer with a learning rate of 0.05 and\n300 steps.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "opt = optax.adam(learning_rate=0.05)\nmax_steps=300\n\n@jax.jit\ndef update_step_jit(i, args):\n    # We loop over this function to optimize the trainable parameters\n    params, opt_state, data, targets, print_training = args\n    loss_val, grads = jax.value_and_grad(loss_fn)(params, data, targets)\n    updates, opt_state = opt.update(grads, opt_state)\n    params = optax.apply_updates(params, updates)\n\n    def print_fn():\n        jax.debug.print(\"Step: {i}  Loss: {loss_val}\", i=i, loss_val=loss_val)\n    # if print_training=True, print the loss every 50 steps\n    jax.lax.cond((jnp.mod(i, 50) == 0 ) & print_training, print_fn, lambda: None)\n    return (params, opt_state, data, targets, print_training)\n\n@jax.jit\ndef optimization_jit(params, data, targets, print_training=False):\n    opt_state = opt.init(params)\n    args = (params, opt_state, jnp.asarray(data), targets, print_training)\n    # We loop over update_step_jit max_steps iterations to optimize the parameters\n    (params, opt_state, _, _, _) = jax.lax.fori_loop(0, max_steps+1, update_step_jit, args)\n    return params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we will train the variational circuit with 4 layers and obtain a\nvector $\\vec{\\theta}$ with the optimized parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "wires=2\nlayers=4\nparams_shape = qml.StronglyEntanglingLayers.shape(n_layers=layers+1,n_wires=wires)\nparams=pnp.random.default_rng().random(size=params_shape)\nbest_params=optimization_jit(params, x_train, jnp.array(y_train), print_training=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you run this yourself, you\\'ll see that the training step with JAX is\nextremely fast! Once the optimized $\\vec{\\theta}$ has been obtained, we\ncan use those parameters to build our fitted version of the function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def evaluate(params, data):\n    y_pred = jax.vmap(quantum_neural_network, in_axes=(None, 0))(params, data)\n    return y_pred\ny_predictions=evaluate(best_params,x_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To compare the fitted function to the exact target function, let\\'s take\na look at the $R^2$ score:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import r2_score\nr2 = round(float(r2_score(y_train, y_predictions)),3)\nprint(\"R^2 Score:\", r2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let\\'s now plot the results to visually check how good our fit is!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig = plt.figure()\n# Target function\nax1 = fig.add_subplot(1, 2, 1, projection='3d')\nax1.plot_surface(x1_mesh,x2_mesh, y_train.reshape(x1_mesh.shape), cmap='viridis')\nax1.set_zlim(0,1)\nax1.set_xlabel('$x$',fontsize=10)\nax1.set_ylabel('$y$',fontsize=10)\nax1.set_zlabel('$f(x,y)$',fontsize=10)\nax1.set_title('Target ')\n\n# Predictions\nax2 = fig.add_subplot(1, 2, 2, projection='3d')\nax2.plot_surface(x1_mesh,x2_mesh, y_predictions.reshape(x1_mesh.shape), cmap='viridis')\nax2.set_zlim(0,1)\nax2.set_xlabel('$x$',fontsize=10)\nax2.set_ylabel('$y$',fontsize=10)\nax2.set_zlabel('$f(x,y)$',fontsize=10)\nax2.set_title(f' Predicted \\nAccuracy: {round(r2*100,3)}%')\n\n# Show the plot\nplt.tight_layout(pad=3.7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cool! We have managed to successfully fit a multidimensional function\nusing a parametrized quantum circuit!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Conclusions\n\nIn this demo, we\\'ve shown how to utilize a variational quantum circuit\nto solve a regression problem for a two-dimensional function. The\nresults show a good agreement with the target function and the model can\nbe trained further, increasing the number of iterations in the training\nto maximize the accuracy. It also paves the way for addressing a\nregression problem for an $N$-dimensional function, as everything\npresented here can be easily generalized. A final check that could be\ndone is to obtain the Fourier coefficients of the trained circuit and\ncompare it with the Fourier series we obtained directly from the target\nfunction.\n\n# References\n\n# About the authors\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}