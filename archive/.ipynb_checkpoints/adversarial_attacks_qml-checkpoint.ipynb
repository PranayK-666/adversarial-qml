{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "787515da-18fb-4dad-9c77-7b40d41f832f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f01f815f250>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import datasets\n",
    "from torchsummary import summary\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "torch.manual_seed(101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7f71711e-1849-49da-baab-60d119d83851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "device = \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c3a7e5e2-9b93-4863-aff8-cbf9e948cbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = [3, 5, 8, 9]\n",
    "n_px = 8\n",
    "\n",
    "train_size = 10000\n",
    "test_size = 2000\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c76ada56-3532-44ea-bb9f-22f011224d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(digits=[3, 5], n_px=8, train_size=1000, test_size=200):\n",
    "    # Load raw data (uint8 [0-255])\n",
    "    mnist_train = datasets.MNIST(root='./data', train=True, download=True)\n",
    "    mnist_test = datasets.MNIST(root='./data', train=False, download=True)\n",
    "\n",
    "    # Helper: filter, sample, normalize, resize\n",
    "    def prepare(data, targets, digits, size, n_px):\n",
    "        # Filter by class\n",
    "        mask = torch.zeros_like(targets, dtype=torch.bool)\n",
    "        for d in digits:\n",
    "            mask |= (targets == d)\n",
    "        data_f = data[mask]\n",
    "        targ_f = targets[mask]\n",
    "\n",
    "        # Sample 'size' examples\n",
    "        idx = torch.randperm(len(data_f))[:size]\n",
    "        imgs = data_f[idx].unsqueeze(1).float() / 255.0  # [size,1,28,28]\n",
    "        labs = targ_f[idx]\n",
    "\n",
    "        # Resize to n_px x n_px\n",
    "        imgs_resized = F.interpolate(imgs, size=(n_px, n_px), mode='bilinear', align_corners=False)\n",
    "        # Map labels to 0..len(digits)-1\n",
    "        labs_mapped = torch.tensor([digits.index(int(l)) for l in labs])\n",
    "        return imgs_resized, labs_mapped\n",
    "\n",
    "    x_train, y_train = prepare(mnist_train.data, mnist_train.targets, digits, train_size, n_px)\n",
    "    x_test, y_test   = prepare(mnist_test.data, mnist_test.targets,   digits, test_size,  n_px)\n",
    "    return (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "7ab3ebf6-6c45-4e6e-ab50-29c05093f6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare dataset\n",
    "(x_train, y_train), (x_test, y_test) = get_dataset(digits=digits, n_px=n_px, train_size=train_size, test_size=test_size)\n",
    "train_loader = DataLoader(TensorDataset(x_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(TensorDataset(x_test,  y_test),  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "74ed7c04-6bdf-4735-93cf-480a4cc0f748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X\n",
      "Shape : (10000, 8, 8)\n",
      "dType : float32\n",
      "pyType: <class 'pennylane.numpy.tensor.tensor'>\n",
      "Y\n",
      "Shape : (10000,)\n",
      "dType : int64\n",
      "pyType: <class 'pennylane.numpy.tensor.tensor'>\n"
     ]
    }
   ],
   "source": [
    "X_train = np.array(np.reshape(x_train, (len(x_train), n_px, n_px)))\n",
    "X_test = np.array(np.reshape(x_test, (len(x_test), n_px, n_px)))\n",
    "Y_train = np.array(y_train)\n",
    "Y_test = np.array(y_test)\n",
    "\n",
    "print('X')\n",
    "print(f'Shape : {X_train.shape}')\n",
    "print(f'dType : {X_train.dtype}')\n",
    "print(f'pyType: {type(X_train)}')\n",
    "\n",
    "print('Y')\n",
    "print(f'Shape : {Y_train.shape}')\n",
    "print(f'dType : {Y_train.dtype}')\n",
    "print(f'pyType: {type(Y_train)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8516a104-31ee-4c95-9617-d89035aae113",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Hyperparameters ####\n",
    "input_dim = n_px * n_px # 16 X 16 pixels\n",
    "num_classes = len(digits)\n",
    "num_layers = 8\n",
    "num_qubits = 8\n",
    "num_reup = 3\n",
    "\n",
    "class QML_classifier(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Class for creating a quantum machine learning (classification) model based on the StronglyEntanglingLayers template.\n",
    "\n",
    "    Args:\n",
    "        input_dim: the dimension of the input samples\n",
    "        output_dim: the dimension of the output, i.e. the numbers of classes\n",
    "        num_qubits: the number of qubits in the circuit\n",
    "        num_layers: the number of layers within the StronglyEntanglingLayers template\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, output_dim, num_qubits, num_layers):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(1337)  # fixed seed for reproducibility\n",
    "        self.num_qubits = num_qubits\n",
    "        self.output_dim = output_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.device = qml.device(\"lightning.qubit\", wires=self.num_qubits)\n",
    "        self.weights_shape = qml.StronglyEntanglingLayers.shape(\n",
    "            n_layers=self.num_layers, n_wires=self.num_qubits\n",
    "        )\n",
    "\n",
    "        @qml.qnode(self.device)\n",
    "        def circuit(inputs, weights, bias):\n",
    "            inputs = torch.reshape(inputs, self.weights_shape)\n",
    "            qml.StronglyEntanglingLayers(\n",
    "                weights=weights * inputs + bias, wires=range(self.num_qubits)\n",
    "            )\n",
    "            return [qml.expval(qml.PauliZ(i)) for i in range(self.output_dim)]\n",
    "\n",
    "        param_shapes = {\"weights\": self.weights_shape, \"bias\": self.weights_shape}\n",
    "        init_vals = {\n",
    "            \"weights\": 0.1 * torch.rand(self.weights_shape),\n",
    "            \"bias\": 0.1 * torch.rand(self.weights_shape),\n",
    "        }\n",
    "\n",
    "        # initialize the quantum circuit\n",
    "        self.qcircuit = qml.qnn.TorchLayer(\n",
    "            qnode=circuit, weight_shapes=param_shapes, init_method=init_vals\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        inputs_stack = torch.hstack([x] * num_reup)\n",
    "        return self.qcircuit(inputs_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "22195e99-8cd0-4229-b3c0-9fec8b8db2d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x200 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show one image from each class (in the train- and testsets, the images are randomly permuted)\n",
    "x_vis = [(x_train[y_train == i])[0].squeeze(0) for i in range(len(digits))]\n",
    "y_vis = digits\n",
    "\n",
    "# later when we train the model we include the predictions as well, so let's just add the functionality here\n",
    "def visualize_data(x, y, pred=None):\n",
    "    n_img = len(x)\n",
    "    fig, axes = plt.subplots(1, n_img, figsize=(2*n_img, 2))\n",
    "    for i in range(n_img):\n",
    "        axes[i].imshow(x[i], cmap=\"gray\")\n",
    "        if pred is None:\n",
    "            axes[i].set_title(\"Label: {}\".format(y[i]))\n",
    "        else:\n",
    "            axes[i].set_title(\"Label: {}, Pred: {}\".format(y[i], pred[i]))\n",
    "    plt.tight_layout(w_pad=2)\n",
    "    # plt.show()\n",
    "\n",
    "visualize_data(x_vis, y_vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "aff37b1f-0c1d-435c-8165-af36703ce3d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training loop for quantum variational classifier (8 qubits, 64 layers)...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[64, 8, 3]' is invalid for input of size 192",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[99]\u001b[39m\u001b[32m, line 60\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m ep \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m0\u001b[39m, epochs):\n\u001b[32m     59\u001b[39m     batch_ind = gen_batches(num_train, num_batches)\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m     \u001b[43mprint_acc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     62\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m it \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_batches):\n\u001b[32m     63\u001b[39m         optimizer.zero_grad()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[99]\u001b[39m\u001b[32m, line 42\u001b[39m, in \u001b[36mprint_acc\u001b[39m\u001b[34m(epoch, max_ep)\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprint_acc\u001b[39m(epoch, max_ep=epochs):\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     predictions_train = [\u001b[43mqml_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m feats_train[:\u001b[32m50\u001b[39m]]\n\u001b[32m     43\u001b[39m     predictions_test = [qml_model(f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m feats_test]\n\u001b[32m     44\u001b[39m     cost_approx_train = loss(torch.stack(predictions_train), labels_train[:\u001b[32m50\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/qml/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/qml/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[97]\u001b[39m\u001b[32m, line 50\u001b[39m, in \u001b[36mQML_classifier.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m     49\u001b[39m     inputs_stack = torch.hstack([x] * num_reup)\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mqcircuit\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs_stack\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/qml/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/qml/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/qml/lib/python3.12/site-packages/pennylane/qnn/torch.py:404\u001b[39m, in \u001b[36mTorchLayer.forward\u001b[39m\u001b[34m(self, inputs)\u001b[39m\n\u001b[32m    401\u001b[39m     inputs = torch.reshape(inputs, (-\u001b[32m1\u001b[39m, inputs.shape[-\u001b[32m1\u001b[39m]))\n\u001b[32m    403\u001b[39m \u001b[38;5;66;03m# calculate the forward pass as usual\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m404\u001b[39m results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_evaluate_qnode\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    407\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m has_batch_dim:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/qml/lib/python3.12/site-packages/pennylane/qnn/torch.py:430\u001b[39m, in \u001b[36mTorchLayer._evaluate_qnode\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    418\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Evaluates the QNode for a single input datapoint.\u001b[39;00m\n\u001b[32m    419\u001b[39m \n\u001b[32m    420\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    424\u001b[39m \u001b[33;03m    tensor: output datapoint\u001b[39;00m\n\u001b[32m    425\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    426\u001b[39m kwargs = {\n\u001b[32m    427\u001b[39m     **{\u001b[38;5;28mself\u001b[39m.input_arg: x},\n\u001b[32m    428\u001b[39m     **{arg: weight.to(x) \u001b[38;5;28;01mfor\u001b[39;00m arg, weight \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.qnode_weights.items()},\n\u001b[32m    429\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m430\u001b[39m res = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mqnode\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    432\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(res, torch.Tensor):\n\u001b[32m    433\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m res.type(x.dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/qml/lib/python3.12/site-packages/pennylane/workflow/qnode.py:905\u001b[39m, in \u001b[36mQNode.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    903\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m qml.capture.enabled():\n\u001b[32m    904\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m capture_qnode(\u001b[38;5;28mself\u001b[39m, *args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m905\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_impl_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/qml/lib/python3.12/site-packages/pennylane/workflow/qnode.py:868\u001b[39m, in \u001b[36mQNode._impl_call\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    865\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_impl_call\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs) -> qml.typing.Result:\n\u001b[32m    866\u001b[39m \n\u001b[32m    867\u001b[39m     \u001b[38;5;66;03m# construct the tape\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m868\u001b[39m     tape = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    870\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.interface == \u001b[33m\"\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    871\u001b[39m         interface = qml.math.get_interface(*args, *\u001b[38;5;28mlist\u001b[39m(kwargs.values()))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/qml/lib/python3.12/site-packages/pennylane/logging/decorators.py:61\u001b[39m, in \u001b[36mlog_string_debug_func.<locals>.wrapper_entry\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     54\u001b[39m     s_caller = \u001b[33m\"\u001b[39m\u001b[33m::L\u001b[39m\u001b[33m\"\u001b[39m.join(\n\u001b[32m     55\u001b[39m         [\u001b[38;5;28mstr\u001b[39m(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m inspect.getouterframes(inspect.currentframe(), \u001b[32m2\u001b[39m)[\u001b[32m1\u001b[39m][\u001b[32m1\u001b[39m:\u001b[32m3\u001b[39m]]\n\u001b[32m     56\u001b[39m     )\n\u001b[32m     57\u001b[39m     lgr.debug(\n\u001b[32m     58\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCalling \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf_string\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00ms_caller\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     59\u001b[39m         **_debug_log_kwargs,\n\u001b[32m     60\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/qml/lib/python3.12/site-packages/pennylane/workflow/qnode.py:854\u001b[39m, in \u001b[36mQNode.construct\u001b[39m\u001b[34m(self, args, kwargs)\u001b[39m\n\u001b[32m    852\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m pldb_device_manager(\u001b[38;5;28mself\u001b[39m.device):\n\u001b[32m    853\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m qml.queuing.AnnotatedQueue() \u001b[38;5;28;01mas\u001b[39;00m q:\n\u001b[32m--> \u001b[39m\u001b[32m854\u001b[39m         \u001b[38;5;28mself\u001b[39m._qfunc_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    856\u001b[39m tape = QuantumScript.from_queue(q, shots)\n\u001b[32m    858\u001b[39m params = tape.get_parameters(trainable_only=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[97]\u001b[39m\u001b[32m, line 31\u001b[39m, in \u001b[36mQML_classifier.__init__.<locals>.circuit\u001b[39m\u001b[34m(inputs, weights, bias)\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;129m@qml\u001b[39m.qnode(\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcircuit\u001b[39m(inputs, weights, bias):\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     inputs = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweights_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m     qml.StronglyEntanglingLayers(\n\u001b[32m     33\u001b[39m         weights=weights * inputs + bias, wires=\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.num_qubits)\n\u001b[32m     34\u001b[39m     )\n\u001b[32m     35\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [qml.expval(qml.PauliZ(i)) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.output_dim)]\n",
      "\u001b[31mRuntimeError\u001b[39m: shape '[64, 8, 3]' is invalid for input of size 192"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t = time.time()\n",
    "#### Hyperparameters ####\n",
    "learning_rate = 0.01\n",
    "epochs = 10\n",
    "batch_size = 20\n",
    "\n",
    "# we use a subset of the training and validation data for this tutorial to speed up the training\n",
    "n_train = 400\n",
    "n_test = 100\n",
    "\n",
    "feats_train = torch.from_numpy(X_train[:n_train]).reshape(n_train, -1).to(device)\n",
    "feats_test = torch.from_numpy(X_test[:n_test]).reshape(n_test, -1).to(device)\n",
    "labels_train = torch.from_numpy(Y_train[:n_train]).to(device)\n",
    "labels_test = torch.from_numpy(Y_test[:n_test]).to(device)\n",
    "num_train = feats_train.shape[0]\n",
    "\n",
    "# initialize the model, loss function and optimization algorithm (Adam optimizer)\n",
    "qml_model = QML_classifier(input_dim, num_classes, num_qubits, num_layers)\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(qml_model.parameters(), lr=learning_rate)\n",
    "num_batches = feats_train.shape[0] // batch_size\n",
    "\n",
    "# set up a measure for performance\n",
    "def accuracy(labels, predictions):\n",
    "    acc = 0\n",
    "    for l, p in zip(labels, predictions):\n",
    "        if torch.argmax(p) == l:\n",
    "            acc += 1\n",
    "    acc = acc / len(labels)\n",
    "    return acc\n",
    "\n",
    "# generate randomly permutated batches to speed up training\n",
    "def gen_batches(num_samples, num_batches):\n",
    "    assert num_samples % num_batches == 0\n",
    "    perm_ind = torch.reshape(torch.randperm(num_samples), (num_batches, -1))\n",
    "    return perm_ind\n",
    "\n",
    "\n",
    "# display accuracy and loss after each epoch (to speed up runtime, only do this for first 100 samples)\n",
    "def print_acc(epoch, max_ep=epochs):\n",
    "    predictions_train = [qml_model(f) for f in feats_train[:50]]\n",
    "    predictions_test = [qml_model(f) for f in feats_test]\n",
    "    cost_approx_train = loss(torch.stack(predictions_train), labels_train[:50])\n",
    "    cost_approx_test = loss(torch.stack(predictions_test), labels_test)\n",
    "    acc_approx_train = accuracy(labels_train[:50], predictions_train)\n",
    "    acc_approx_test = accuracy(labels_test, predictions_test)\n",
    "    print(\n",
    "        f\"Epoch {epoch}/{max_ep} | Approx Cost (train): {cost_approx_train:0.7f} | Cost (val): {cost_approx_test:0.7f} |\"\n",
    "        f\" Approx Acc train: {acc_approx_train:0.7f} | Acc val: {acc_approx_test:0.7f}\"\n",
    "    )\n",
    "\n",
    "print(\n",
    "    f\"Starting training loop for quantum variational classifier ({num_qubits} qubits, {num_layers} layers)...\"\n",
    ")\n",
    "\n",
    "# optimize over model parameters for given number of epochs\n",
    "for ep in range(0, epochs):\n",
    "    batch_ind = gen_batches(num_train, num_batches)\n",
    "    print_acc(epoch=ep)\n",
    "\n",
    "    for it in range(num_batches):\n",
    "        optimizer.zero_grad()\n",
    "        feats_train_batch = feats_train[batch_ind[it]]\n",
    "        labels_train_batch = labels_train[batch_ind[it]]\n",
    "\n",
    "        outputs = [qml_model(f) for f in feats_train_batch]\n",
    "        batch_loss = loss(torch.stack(outputs), labels_train_batch)\n",
    "        # if REG:\n",
    "        #    loss = loss + lipschitz_regularizer(regularization_rate, model.qcircuit.weights)\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "print_acc(epochs)\n",
    "\n",
    "elapsed = time.time() - t\n",
    "print(f\"\\nTime elapsed: {elapsed} s.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d08274e8-8061-4a4a-bcab-6ed2741511ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QML_classifier Summary:\n",
      "  Input dim:    256\n",
      "  Output dim:   4\n",
      "  Num qubits:   8\n",
      "  Num layers:   32\n",
      "  Num reup:     3\n",
      "\n",
      "Quantum Circuit Layer:\n",
      "  weights    shape: (32, 8, 3) - 768 params\n",
      "  bias       shape: (32, 8, 3) - 768 params\n",
      "\n",
      "Total trainable parameters: 1536\n"
     ]
    }
   ],
   "source": [
    "def print_qml_classifier_summary(model):\n",
    "    print(\"QML_classifier Summary:\")\n",
    "    print(f\"  Input dim:    {input_dim}\")\n",
    "    print(f\"  Output dim:   {model.output_dim}\")\n",
    "    print(f\"  Num qubits:   {model.num_qubits}\")\n",
    "    print(f\"  Num layers:   {model.num_layers}\")\n",
    "    print(f\"  Num reup:     {num_reup}\")\n",
    "    print()\n",
    "\n",
    "    print(\"Quantum Circuit Layer:\")\n",
    "    total_params = 0\n",
    "    for name, param in model.qcircuit.named_parameters():\n",
    "        print(f\"  {name:<10} shape: {tuple(param.shape)} - {param.numel()} params\")\n",
    "        total_params += param.numel()\n",
    "\n",
    "    print(f\"\\nTotal trainable parameters: {total_params}\")\n",
    "\n",
    "# Usage\n",
    "model = QML_classifier(input_dim, num_classes, num_qubits, num_layers)\n",
    "print_qml_classifier_summary(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f639010b-73f3-43db-8b6e-527f9b182549",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (qml)",
   "language": "python",
   "name": "qml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
