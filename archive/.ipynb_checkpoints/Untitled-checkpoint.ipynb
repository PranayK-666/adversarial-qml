{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e50958fe-79a2-4a1b-a551-0ddd99075349",
   "metadata": {},
   "source": [
    "# MNIST classification classical model (Pytorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47338af6-016b-4c69-82df-f94d628653b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "import torch # to use PennyLane TorchLayer class to perform circuit operations and optimisations with PyTorch backend\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from utils.tools import get_dataset, visualise_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ccec005-8e85-421b-b726-c5211fee107c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aae2d8f0-9654-4d6b-b151-48bee2468adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_px = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8dae8cb4-2589-4240-ab80-934b3ec9e82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, n_px):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(n_px*n_px, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 10),\n",
    "            nn.Softmax(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bb88cfe-ce3c-43da-a87f-9549377d99b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=8, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=8, out_features=10, bias=True)\n",
      "    (3): Softmax(dim=None)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork(n_px).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2813c2a0-8a5e-4327-a5b5-e2386bd345d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2af5cd14-9e46-4a0c-a7a5-bea3c628379d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Linear.__init__() missing 2 required positional arguments: 'in_features' and 'out_features'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m model = \u001b[43mCNN\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_px\u001b[49m\u001b[43m)\u001b[49m.to(device)\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(model)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mCNN.__init__\u001b[39m\u001b[34m(self, n_px)\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m()\n\u001b[32m      4\u001b[39m \u001b[38;5;28mself\u001b[39m.flatten = nn.Flatten()\n\u001b[32m      5\u001b[39m \u001b[38;5;28mself\u001b[39m.convoluitonal_stack = nn.Sequential(\n\u001b[32m      6\u001b[39m     nn.Conv2d(\u001b[32m1\u001b[39m, \u001b[32m32\u001b[39m, kernel_size=\u001b[32m3\u001b[39m),\n\u001b[32m      7\u001b[39m     nn.ReLU(),\n\u001b[32m      8\u001b[39m     nn.Conv2d(\u001b[32m32\u001b[39m, \u001b[32m64\u001b[39m, kernel_size=\u001b[32m3\u001b[39m),\n\u001b[32m      9\u001b[39m     nn.ReLU(),\n\u001b[32m     10\u001b[39m     nn.MaxPool2d(\u001b[32m2\u001b[39m, \u001b[32m2\u001b[39m),\n\u001b[32m     11\u001b[39m     nn.Dropout(\u001b[32m0.25\u001b[39m),\n\u001b[32m     12\u001b[39m     nn.Flatten(),\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     \u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m     14\u001b[39m     nn.ReLU(),\n\u001b[32m     15\u001b[39m     nn.Linear(\u001b[32m8\u001b[39m, \u001b[32m10\u001b[39m),\n\u001b[32m     16\u001b[39m     nn.Softmax(),\n\u001b[32m     17\u001b[39m )\n",
      "\u001b[31mTypeError\u001b[39m: Linear.__init__() missing 2 required positional arguments: 'in_features' and 'out_features'"
     ]
    }
   ],
   "source": [
    "model = CNN(n_px).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762d8355-b4fb-4110-9e48-ae1af297e221",
   "metadata": {},
   "source": [
    "### CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b2aeb59-24c7-4c4c-b9db-4fef8cc158b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36b08d4e-5b7a-47f0-84ba-644b84a900dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes=10):\n",
    "        \"\"\"\n",
    "        Define the layers of the convolutional neural network.\n",
    "\n",
    "        Parameters:\n",
    "            in_channels: int\n",
    "                The number of channels in the input image. For MNIST, this is 1 (grayscale images).\n",
    "            num_classes: int\n",
    "                The number of classes we want to predict, in our case 10 (digits 0 to 9).\n",
    "        \"\"\"\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        # First convolutional layer: 1 input channel, 8 output channels, 3x3 kernel, stride 1, padding 1\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=8, kernel_size=3, stride=1, padding=1)\n",
    "        # Max pooling layer: 2x2 window, stride 2\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # Second convolutional layer: 8 input channels, 16 output channels, 3x3 kernel, stride 1, padding 1\n",
    "        self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        # Fully connected layer: 16*7*7 input features (after two 2x2 poolings), 10 output features (num_classes)\n",
    "        self.fc1 = nn.Linear(16 * 7 * 7, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Define the forward pass of the neural network.\n",
    "\n",
    "        Parameters:\n",
    "            x: torch.Tensor\n",
    "                The input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor\n",
    "                The output tensor after passing through the network.\n",
    "        \"\"\"\n",
    "        x = F.relu(self.conv1(x))  # Apply first convolution and ReLU activation\n",
    "        x = self.pool(x)           # Apply max pooling\n",
    "        x = F.relu(self.conv2(x))  # Apply second convolution and ReLU activation\n",
    "        x = self.pool(x)           # Apply max pooling\n",
    "        x = x.reshape(x.shape[0], -1)  # Flatten the tensor\n",
    "        x = self.fc1(x)            # Apply fully connected layer\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a681221-fbb9-44ad-a2e5-0f9083108cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64dd14a1-f196-47f6-80f6-12e9edccd519",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x200 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "digits = [0,1,2,3]\n",
    "n_px = 16\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = get_dataset(digits=digits, n_px=n_px)\n",
    "\n",
    "# show one image from each class\n",
    "x_vis = [(x_train[y_train==digit])[np.random.choice(range(10))] for digit in digits] \n",
    "y_vis = range(len(digits))\n",
    "\n",
    "visualise_data(digits, x_vis, y_vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9b755d0-b480-4f9f-bae1-8770516e5d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = n_px * n_px  # 28x28 pixels (not directly used in CNN)\n",
    "num_classes = len(digits)  # digits 0-9\n",
    "learning_rate = 0.001\n",
    "batch_size = 32\n",
    "num_epochs = 10  # Reduced for demonstration purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46052996-9942-492c-8a64-d3c2829f33c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.MNIST(root=\"dataset/\", download=True, train=True, transform=transforms.ToTensor())\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root=\"dataset/\", download=True, train=False, transform=transforms.ToTensor())\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5dbfd8c1-51ee-4394-914a-0b1d2e8f711e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN(in_channels=1, num_classes=num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce01aee0-2fe0-4f79-b5e9-182c25aff88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6dbeb7ea-a29d-44ac-86a0-6589c390fc1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 938/938 [00:10<00:00, 88.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 938/938 [00:07<00:00, 119.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 938/938 [00:07<00:00, 123.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 938/938 [00:07<00:00, 122.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 938/938 [00:08<00:00, 115.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 938/938 [00:08<00:00, 113.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 938/938 [00:08<00:00, 115.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 938/938 [00:08<00:00, 112.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 938/938 [00:08<00:00, 115.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 938/938 [00:07<00:00, 123.71it/s]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}]\")\n",
    "    for batch_index, (data, targets) in enumerate(tqdm(train_loader)):\n",
    "        # Move data and targets to the device (GPU/CPU)\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # Forward pass: compute the model output\n",
    "        scores = model(data)\n",
    "        loss = criterion(scores, targets)\n",
    "\n",
    "        # Backward pass: compute the gradients\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Optimization step: update the model parameters\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3855cefa-cea0-4b17-bdc6-cc889936cde3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking accuracy on training data\n",
      "Got 59488/60000 with accuracy 99.15%\n",
      "Checking accuracy on test data\n",
      "Got 9869/10000 with accuracy 98.69%\n"
     ]
    }
   ],
   "source": [
    "def check_accuracy(loader, model):\n",
    "    \"\"\"\n",
    "    Checks the accuracy of the model on the given dataset loader.\n",
    "\n",
    "    Parameters:\n",
    "        loader: DataLoader\n",
    "            The DataLoader for the dataset to check accuracy on.\n",
    "        model: nn.Module\n",
    "            The neural network model.\n",
    "    \"\"\"\n",
    "    if loader.dataset.train:\n",
    "        print(\"Checking accuracy on training data\")\n",
    "    else:\n",
    "        print(\"Checking accuracy on test data\")\n",
    "\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            # Forward pass: compute the model output\n",
    "            scores = model(x)\n",
    "            _, predictions = scores.max(1)  # Get the index of the max log-probability\n",
    "            num_correct += (predictions == y).sum()  # Count correct predictions\n",
    "            num_samples += predictions.size(0)  # Count total samples\n",
    "\n",
    "        # Calculate accuracy\n",
    "        accuracy = float(num_correct) / float(num_samples) * 100\n",
    "        print(f\"Got {num_correct}/{num_samples} with accuracy {accuracy:.2f}%\")\n",
    "    \n",
    "    model.train()  # Set the model back to training mode\n",
    "\n",
    "# Final accuracy check on training and test sets\n",
    "check_accuracy(train_loader, model)\n",
    "check_accuracy(test_loader, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccb5d6e-1e88-46ca-bbe9-4f36457a410f",
   "metadata": {},
   "source": [
    "### PGD attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6080e11c-2f22-434f-a79a-69fa947fe0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple implementation of projected gradient descent (PGD) attack (without randomized starting points — cf. BIM)\n",
    "# for an introduction to PGD, see https://adversarial-ml-tutorial.org/adversarial_examples/#projected-gradient-descent\n",
    "def PGD(model, feats, labels, epsilon=0.1, alpha=0.1, num_iter=10):\n",
    "\n",
    "    # initialize image perturbations with zero\n",
    "    delta = torch.zeros_like(feats, requires_grad=True)\n",
    "    for t in range(num_iter):\n",
    "        feats_adv = feats + delta\n",
    "        outputs = [model(f) for f in feats_adv]\n",
    "\n",
    "        # forward & backward pass through the model, accumulating gradients\n",
    "        l = loss(torch.stack(outputs), labels)\n",
    "        l.backward()\n",
    "\n",
    "        # use gradients with respect to inputs and clip the results to lie within the epsilon boundary\n",
    "        delta.data = (delta + alpha * delta.grad.detach().sign()).clamp(-epsilon, epsilon)\n",
    "        delta.grad.zero_()\n",
    "    return delta.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3335e377-051f-4cbf-ac47-2309807e4d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "perturbations = PGD(model=model, feats=x_vis_torch, labels=y_vis_torch, epsilon=0.2) # even smaller values of epsilon do some damage (epsilon > 0.04)\n",
    "perturbed_x = x_vis_torch + perturbations\n",
    "\n",
    "# check model performance\n",
    "adversarial_preds = [model(f) for f in perturbed_x]\n",
    "adversarial_class_output = [torch.argmax(p) for p in adversarial_preds]\n",
    "\n",
    "# visualise_data(digits, perturbed_x.reshape(-1, n_px, n_px), y_vis, adversarial_class_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c058852c-abd3-4d37-b28f-a9ead0757d8e",
   "metadata": {},
   "source": [
    "## Tutorial on adversarial robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f46d42-b99a-4e6b-9b17-ed7f0a1d1385",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (qml)",
   "language": "python",
   "name": "qml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
