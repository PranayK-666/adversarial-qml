{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell is added by sphinx-gallery\n",
    "# It can be customized to whatever you like\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adversarial attacks and robustness for quantum machine learning\n",
    "===============================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This demo is based on the paper *A Comparative Analysis of Adversarial\n",
    "Robustness for Quantum and Classical Machine Learning Models* by M.\n",
    "Wendlinger, K. Tscharke and P. Debus, which dives into the world of\n",
    "adversarial attacks on quantum computers to find relations to classical\n",
    "adversarial machine learning. In the following, we briefly cover the\n",
    "necessary theoretical baselines of adversarial attacks on classification\n",
    "networks before giving a hands-on example of how to construct such an\n",
    "attack and how we can make QML models more robust. Let's go!\n",
    "\n",
    "![](../_static/demonstration_assets/adversarial_attacks_QML/QAML_overview.png){.align-center\n",
    "width=\"80.0%\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are adversarial attacks?\n",
    "\n",
    "Adversarial attacks are small, carefully crafted perturbations to input\n",
    "data that lead to high rates of misclassification for machine learning\n",
    "models, while the data seems to be identical to the original for human\n",
    "observers. In particular, images are sensitive to this type of attack,\n",
    "as small manipulations of the pixels can fool a classifier without any\n",
    "changes being visible to humans. A typical example of an adversarial\n",
    "attack is shown in the picture above, where an image of a panda is\n",
    "manipulated by adding a small noise to each pixel. The original image is\n",
    "correctly classified as \\\"Panda,\\\" while the image with tiny\n",
    "manipulations is falsely classified as \\\"Gibbon\\\" (the noise is\n",
    "magnified in the figure so we can actually see it). This example is\n",
    "adapted from a famous paper showing the vulnerability of classical\n",
    "machine learning models to adversarial attacks.\n",
    "\n",
    "Mathematically, the goal of an (untargeted) attack is to achieve a\n",
    "misclassification of the model such that a sample $x$ leads to a\n",
    "predicted label $y' \\neq y$ that is not the true label $y.$ This is\n",
    "achieved by finding the perturbation $\\delta\\in\\Delta$ to the original\n",
    "input that maximizes the loss of the true class. For a loss function\n",
    "$\\mathcal{L},$ a model $f: \\mathbb{R}^{D} \\to \\mathbb{R}^K$ (mapping\n",
    "$D$-dimensional input to\n",
    "[softmax](https://en.wikipedia.org/wiki/Softmax_function) probability\n",
    "scores of $K$ classes with model parameters $\\theta^*$), the objective\n",
    "of the untargeted attack is:\n",
    "\n",
    "$$\\delta \\equiv \\; \\underset{\\delta^{\\prime} \\in \\Delta}{\\operatorname{argmax}} \\;\\mathcal{L}\\left(f\\left(x+\\delta^{\\prime} ; \\theta^*\\right), y\\right).$$\n",
    "\n",
    "Later, when we show how to actually construct such an attack, we will\n",
    "revisit this equation. For an adversarial attack to be considered\n",
    "useful, it must hold that the modifications to the input elements are\n",
    "imperceptible, i.e. that\n",
    "$\\Delta=\\{\\delta \\in \\mathbb{R}^{D}: \\| \\delta\\|_{\\infty} \\le \\varepsilon\\},$\n",
    "where $\\varepsilon$ is some small bound, typically below $0.1.$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why are adversarial attacks dangerous?\n",
    "\n",
    "Machine learning (ML) is becoming essential in many security-critical\n",
    "applications, for example in\n",
    "[cybersecurity](https://arxiv.org/abs/1901.03407), the [medical\n",
    "sector](https://arxiv.org/abs/1606.05718), or [autonomous\n",
    "vehicles](https://arxiv.org/abs/1604.07316). In cybersecurity, ML models\n",
    "are used, amongst others, for malware detection or the prevention of\n",
    "attacks on networks. Given their critical roles, any successful attack\n",
    "of these ML models can lead to severe consequences, ranging from\n",
    "financial losses and privacy violations to threats to human life. As we\n",
    "have seen above, adversarial attacks are imperceptible to humans, and\n",
    "hence difficult to detect. For this reason, it is essential that ML\n",
    "models in security-critical applications are robust against these types\n",
    "of attacks.\n",
    "\n",
    "[Quantum machine learning\n",
    "(QML)](%5B/whatisqml%5D(https://pennylane.ai/qml/whatisqml/)) has been\n",
    "shown to have theoretical advantages over classical ML methods and is\n",
    "becoming increasingly popular. However, first works in this direction\n",
    "suggest that QML suffers from the same vulnerabilities as classical ML.\n",
    "How the vulnerability of QML models relates to classical models and how\n",
    "robust the models are in comparison is evaluated in. But enough talk,\n",
    "let's see how we can actually attack a QML model!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's see this in action!\n",
    "\n",
    "## Setting up the environment\n",
    "\n",
    "For this tutorial, we will use the PennyLane\n",
    "`~pennylane.qnn.TorchLayer`{.interpreted-text role=\"class\"} class to\n",
    "perform circuit operations and optimizations with the PyTorch backend.\n",
    "Thus, we need to import the torch library alongside PennyLane:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "import torch\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of the dataset\n",
    "\n",
    "As in the paper, we make use of the\n",
    "[PlusMinus](https://pennylane.ai/datasets/single-dataset/plus-minus)\n",
    "dataset (available via [PennyLane\n",
    "Datasets](https://pennylane.ai/datasets/)), which serves as a good\n",
    "baseline for evaluating a QML image classification model's ability to\n",
    "find useful features in the input. It also allows us to define the\n",
    "usefulness of attacks on the QML model while being low-dimensional\n",
    "enough to perform scalable training (more info on the dataset can be\n",
    "found in). It consists of four classes of $16\\times16$ pixel grayscale\n",
    "images which show one of the four symbols $\\{+,-,\\vdash,\\dashv\\}.$ Below\n",
    "we visualize one sample of each class to get an understanding of the\n",
    "dataset.\n",
    "\n",
    "The data can be loaded directly from [PennyLane\n",
    "Datasets](https://pennylane.ai/datasets/) for easy integration into\n",
    "PennyLane circuits and optimization code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x200 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# we can use the dataset hosted on PennyLane\n",
    "[pm] = qml.data.load('other', name='plus-minus')\n",
    "\n",
    "X_train = pm.img_train  # shape (1000,16,16)\n",
    "X_test = pm.img_test  # shape (200,16,16)\n",
    "Y_train = pm.labels_train  # shape (1000,)\n",
    "Y_test = pm.labels_test  # shape (200,)\n",
    "\n",
    "# show one image from each class (in the train- and testsets, the images are randomly permuted)\n",
    "x_vis = [\n",
    "    (X_train[Y_train == 0])[0],\n",
    "    (X_train[Y_train == 1])[0],\n",
    "    (X_train[Y_train == 2])[0],\n",
    "    (X_train[Y_train == 3])[0],\n",
    "]\n",
    "y_vis = [0, 1, 2, 3]\n",
    "\n",
    "\n",
    "# later when we train the model we include the predictions as well, so let's just add the functionality here\n",
    "def visualize_data(x, y, pred=None):\n",
    "    n_img = len(x)\n",
    "    labels_list = [\"\\u2212\", \"\\u002b\", \"\\ua714\", \"\\u02e7\"]\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(8, 2))\n",
    "    for i in range(n_img):\n",
    "        axes[i].imshow(x[i], cmap=\"gray\")\n",
    "        if pred is None:\n",
    "            axes[i].set_title(\"Label: {}\".format(labels_list[y[i]]))\n",
    "        else:\n",
    "            axes[i].set_title(\"Label: {}, Pred: {}\".format(labels_list[y[i]], labels_list[pred[i]]))\n",
    "    plt.tight_layout(w_pad=2)\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "visualize_data(x_vis, y_vis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the QML circuit for classification\n",
    "\n",
    "We will make use of a\n",
    "`data-reuploading <tutorial_data_reuploading_classifier>`{.interpreted-text\n",
    "role=\"doc\"} scheme to encode the 256 input pixels into the latent space\n",
    "of the quantum classifier. To this end, the\n",
    "`~pennylane.StronglyEntanglingLayers`{.interpreted-text role=\"class\"}\n",
    "template provides an easy-to-use structure for the circuit design. The\n",
    "output of our classifier is a four-dimensional vector resulting from\n",
    "Pauli Z oberservables along the first four qubits. These outputs\n",
    "(unnormalized probability scores --- i.e. logits) are then used in the\n",
    "[CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)\n",
    "function to optimize the model parameters. Together with the PyTorch\n",
    "integration mentioned above, the classifier code looks like the\n",
    "following:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "#### Hyperparameters ####\n",
    "input_dim = 256\n",
    "num_classes = 4\n",
    "num_layers = 32\n",
    "num_qubits = 8\n",
    "num_reup = 3\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "\n",
    "class QML_classifier(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Class for creating a quantum machine learning (classification) model based on the StronglyEntanglingLayers template.\n",
    "\n",
    "    Args:\n",
    "        input_dim: the dimension of the input samples\n",
    "        output_dim: the dimension of the output, i.e. the numbers of classes \n",
    "        num_qubits: the number of qubits in the circuit\n",
    "        num_layers: the number of layers within the StronglyEntanglingLayers template\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, output_dim, num_qubits, num_layers):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(1337)  # fixed seed for reproducibility\n",
    "        self.num_qubits = num_qubits\n",
    "        self.output_dim = output_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.device = qml.device(\"lightning.qubit\", wires=self.num_qubits)\n",
    "        self.weights_shape = qml.StronglyEntanglingLayers.shape(\n",
    "            n_layers=self.num_layers, n_wires=self.num_qubits\n",
    "        )\n",
    "\n",
    "        @qml.qnode(self.device)\n",
    "        def circuit(inputs, weights, bias):\n",
    "            inputs = torch.reshape(inputs, self.weights_shape)\n",
    "            qml.StronglyEntanglingLayers(\n",
    "                weights=weights * inputs + bias, wires=range(self.num_qubits)\n",
    "            )\n",
    "            return [qml.expval(qml.PauliZ(i)) for i in range(self.output_dim)]\n",
    "\n",
    "        param_shapes = {\"weights\": self.weights_shape, \"bias\": self.weights_shape}\n",
    "        init_vals = {\n",
    "            \"weights\": 0.1 * torch.rand(self.weights_shape),\n",
    "            \"bias\": 0.1 * torch.rand(self.weights_shape),\n",
    "        }\n",
    "\n",
    "        # initialize the quantum circuit\n",
    "        self.qcircuit = qml.qnn.TorchLayer(\n",
    "            qnode=circuit, weight_shapes=param_shapes, init_method=init_vals\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        inputs_stack = torch.hstack([x] * num_reup)\n",
    "        return self.qcircuit(inputs_stack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the classifier\n",
    "\n",
    "The 'test' set will be used as validation in each training step to\n",
    "verify the generalization capabilities of our classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training loop for quantum variational classifier (8 qubits, 32 layers)...\n",
      "Epoch 0/4 | Approx Cost (train): 1.3883214 | Cost (val): 1.3875420 | Approx Acc train: 0.1600000 | Acc val: 0.1200000\n"
     ]
    }
   ],
   "source": [
    "#### Hyperparameters ####\n",
    "learning_rate = 0.1\n",
    "epochs = 4\n",
    "batch_size = 20\n",
    "\n",
    "# we use a subset of the training and validation data for this tutorial to speed up the training\n",
    "feats_train = torch.from_numpy(X_train[:200]).reshape(200, -1).to(device)\n",
    "feats_test = torch.from_numpy(X_test[:50]).reshape(50, -1).to(device)\n",
    "labels_train = torch.from_numpy(Y_train[:200]).to(device)\n",
    "labels_test = torch.from_numpy(Y_test[:50]).to(device)\n",
    "num_train = feats_train.shape[0]\n",
    "\n",
    "# initialize the model, loss function and optimization algorithm (Adam optimizer)\n",
    "qml_model = QML_classifier(input_dim, num_classes, num_qubits, num_layers)\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(qml_model.parameters(), lr=learning_rate)\n",
    "num_batches = feats_train.shape[0] // batch_size\n",
    "\n",
    "\n",
    "# set up a measure for performance\n",
    "def accuracy(labels, predictions):\n",
    "    acc = 0\n",
    "    for l, p in zip(labels, predictions):\n",
    "        if torch.argmax(p) == l:\n",
    "            acc += 1\n",
    "    acc = acc / len(labels)\n",
    "    return acc\n",
    "\n",
    "\n",
    "# generate randomly permutated batches to speed up training\n",
    "def gen_batches(num_samples, num_batches):\n",
    "    assert num_samples % num_batches == 0\n",
    "    perm_ind = torch.reshape(torch.randperm(num_samples), (num_batches, -1))\n",
    "    return perm_ind\n",
    "\n",
    "\n",
    "# display accuracy and loss after each epoch (to speed up runtime, only do this for first 100 samples)\n",
    "def print_acc(epoch, max_ep=4):\n",
    "    predictions_train = [qml_model(f) for f in feats_train[:50]]\n",
    "    predictions_test = [qml_model(f) for f in feats_test]\n",
    "    cost_approx_train = loss(torch.stack(predictions_train), labels_train[:50])\n",
    "    cost_approx_test = loss(torch.stack(predictions_test), labels_test)\n",
    "    acc_approx_train = accuracy(labels_train[:50], predictions_train)\n",
    "    acc_approx_test = accuracy(labels_test, predictions_test)\n",
    "    print(\n",
    "        f\"Epoch {epoch}/{max_ep} | Approx Cost (train): {cost_approx_train:0.7f} | Cost (val): {cost_approx_test:0.7f} |\"\n",
    "        f\" Approx Acc train: {acc_approx_train:0.7f} | Acc val: {acc_approx_test:0.7f}\"\n",
    "    )\n",
    "\n",
    "\n",
    "print(\n",
    "    f\"Starting training loop for quantum variational classifier ({num_qubits} qubits, {num_layers} layers)...\"\n",
    ")\n",
    "\n",
    "# optimize over model parameters for given number of epochs\n",
    "for ep in range(0, epochs):\n",
    "    batch_ind = gen_batches(num_train, num_batches)\n",
    "    print_acc(epoch=ep)\n",
    "\n",
    "    for it in range(num_batches):\n",
    "        optimizer.zero_grad()\n",
    "        feats_train_batch = feats_train[batch_ind[it]]\n",
    "        labels_train_batch = labels_train[batch_ind[it]]\n",
    "\n",
    "        outputs = [qml_model(f) for f in feats_train_batch]\n",
    "        batch_loss = loss(torch.stack(outputs), labels_train_batch)\n",
    "        # if REG:\n",
    "        #    loss = loss + lipschitz_regularizer(regularization_rate, model.qcircuit.weights)\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "print_acc(epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation - benign data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# show accuracy\n",
    "x_vis_torch = torch.from_numpy(np.array(x_vis).reshape(4, -1))\n",
    "y_vis_torch = torch.from_numpy(np.array(y_vis))\n",
    "benign_preds = [qml_model(f) for f in x_vis_torch]\n",
    "\n",
    "benign_class_output = [torch.argmax(p) for p in benign_preds]\n",
    "visualize_data(x_vis, y_vis, benign_class_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's break stuff!\n",
    "\n",
    "As described before, the mathematical notation for an adversarial attack\n",
    "is as follows:\n",
    "\n",
    "$$\\delta \\equiv \\; \\underset{\\delta^{\\prime} \\in \\Delta}{\\operatorname{argmax}} \\;\\mathcal{L}\\left(f\\left(x+\\delta^{\\prime} ; \\theta^*\\right), y\\right).$$\n",
    "\n",
    "This equation can be summarized in a simple step-by-step recipe. In\n",
    "basic terms, we perform a forward and backward pass through the model\n",
    "and loss function (just like we do during training) for the specific\n",
    "samples that we want to find perturbations for. The difference to\n",
    "training steps is that the gradients we calculate are **with respect to\n",
    "the input features** of the data samples. Using these gradients, we find\n",
    "the direction of ascent in the loss function (as we want to force high\n",
    "losses, i.e. bad model performance for the specific samples). Finally,\n",
    "we clip the delta (perturbations), such that they lie within the epsilon\n",
    "boundary we set beforehand (this is just some hyperparameter wich limits\n",
    "the attack strength). All of these steps can be seen in the code below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# simple implementation of projected gradient descent (PGD) attack (without randomized starting points — cf. BIM)\n",
    "# for an introduction to PGD, see https://adversarial-ml-tutorial.org/adversarial_examples/#projected-gradient-descent\n",
    "def PGD(model, feats, labels, epsilon=0.1, alpha=0.01, num_iter=10):\n",
    "\n",
    "    # initialize image perturbations with zero\n",
    "    delta = torch.zeros_like(feats, requires_grad=True)\n",
    "    for t in range(num_iter):\n",
    "        feats_adv = feats + delta\n",
    "        outputs = [model(f) for f in feats_adv]\n",
    "\n",
    "        # forward & backward pass through the model, accumulating gradients\n",
    "        l = loss(torch.stack(outputs), labels)\n",
    "        l.backward()\n",
    "\n",
    "        # use gradients with respect to inputs and clip the results to lie within the epsilon boundary\n",
    "        delta.data = (delta + alpha * delta.grad.detach().sign()).clamp(-epsilon, epsilon)\n",
    "        delta.grad.zero_()\n",
    "    return delta.detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation --- model under attack\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "perturbations = PGD(model=qml_model, feats=x_vis_torch, labels=y_vis_torch, epsilon=0.1)\n",
    "perturbed_x = x_vis_torch + perturbations\n",
    "\n",
    "# check model performance\n",
    "adversarial_preds = [qml_model(f) for f in perturbed_x]\n",
    "adversarial_class_output = [torch.argmax(p) for p in adversarial_preds]\n",
    "\n",
    "visualize_data(perturbed_x.reshape(-1, 16, 16), y_vis, adversarial_class_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the devastating effect of a simple PGD (projected gradient\n",
    "descent) attack using a perturbation strength $\\varepsilon=0.1,$ where\n",
    "the model misclassifies each of the four samples we used for\n",
    "visualization of the dataset. For humans, the images are still very\n",
    "easily classifiable, the perturbations look mostly like random noise\n",
    "added to the images. All in all, the accuracy of the model for the\n",
    "perturbed trainset decreases to around $0.1,$ so almost all samples of\n",
    "the dataset are misclassified!\n",
    "\n",
    "Using the code above, you can try the attack on your own and check which\n",
    "samples are more robust against attacks and which labels the model\n",
    "(wrongly) assigns to the data. The remaining question is: Can we defend\n",
    "against such attacks? How can we train our (QML) models to be more\n",
    "robust against adversarial attacks? This is a field of much active\n",
    "research and in general a very hard problem to solve for different kinds\n",
    "of attacks, model architectures and input data, but a very simple\n",
    "approach is given below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Increasing the robustness of QML models\n",
    "\n",
    "The easiest way to make our model aware of slight modifications is to\n",
    "simply include the perturbed images into the training dataset (this can\n",
    "be seen as data augmentation and is often done in classical machine\n",
    "learning using noise). That way, the model is expected to be more robust\n",
    "against samples that were constructed using the same attack without\n",
    "changing anything in the model architecture itself; this method is\n",
    "called **adversarial (re)training**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "adv_dataset = (\n",
    "    PGD(model=qml_model, feats=feats_train[:20], labels=labels_train[:20], epsilon=0.1)\n",
    "    + feats_train[:20]\n",
    ")\n",
    "\n",
    "feats_retrain = torch.cat((feats_train, adv_dataset))\n",
    "labels_retrain = torch.cat((labels_train, labels_train[:20]))\n",
    "epochs_retraining = 2\n",
    "\n",
    "for ep in range(0, epochs_retraining):\n",
    "    batch_ind = gen_batches(num_train, num_batches)\n",
    "    print_acc(epoch=ep, max_ep=2)\n",
    "\n",
    "    for it in range(num_batches):\n",
    "        optimizer.zero_grad()\n",
    "        feats_train_batch = feats_retrain[batch_ind[it]]\n",
    "        labels_train_batch = labels_retrain[batch_ind[it]]\n",
    "\n",
    "        outputs = [qml_model(f) for f in feats_train_batch]\n",
    "        batch_loss = loss(torch.stack(outputs), labels_train_batch)\n",
    "        # if REG:\n",
    "        #    loss = loss + lipschitz_regularizer(regularization_rate, model.qcircuit.weights)\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "print_acc(epochs_retraining, max_ep=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of the retrained model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "adversarial_preds = [qml_model(f) for f in perturbed_x]\n",
    "adversarial_class_output = [torch.argmax(p) for p in adversarial_preds]\n",
    "\n",
    "visualize_data(perturbed_x.reshape(-1, 16, 16), y_vis, adversarial_class_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the model now correctly classifies three out of the four\n",
    "perturbed input images. As before, you can adapt the code above and test\n",
    "the retrained model for the whole dataset to see how much the accuracy\n",
    "under attack improves overall.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this demo we saw how to perform adversarial attacks on quantum\n",
    "variational classification models. The resulting perturbed images (which\n",
    "--- for humans --- still closely resemble the original ones) result in\n",
    "large misclassification rates of the QML models, showing the\n",
    "vulnerability of such models to adversarial attacks. Consequently, we\n",
    "showcased one possibility to increase the adversarial robustness using\n",
    "adversarial retraining, which led the model under attack to perform\n",
    "better.\n",
    "\n",
    "While this method does seem useful, it needs to be noted that this is a\n",
    "very basic approach (just include the pertubed images in the trainset,\n",
    "duh!) and might not work equally well for a different attack or slightly\n",
    "changed epsilon values. There are approaches (e.g. using Lipschitz\n",
    "regularization as shown in) that have the potential to be more effective\n",
    "in increasing the robustness, but this is still a field of ongoing\n",
    "research, and many different approaches are waiting to be discovered!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About the authors\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
