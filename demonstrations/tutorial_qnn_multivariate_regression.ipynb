{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell is added by sphinx-gallery\n",
    "# It can be customized to whatever you like\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multidimensional regression with a variational quantum circuit\n",
    "===========================================================\n",
    "\n",
    "In this tutorial, we show how to use a variational quantum circuit to\n",
    "fit the simple multivariate function\n",
    "\n",
    "$$f(x_1, x_2) = \\frac{1}{2} \\left( x_1^2 + x_2^2 \\right).$$\n",
    "\n",
    "In it has been shown that, under some conditions, there exist\n",
    "variational quantum circuits that are expressive enough to realize any\n",
    "possible set of Fourier coefficients. We will use a simple two-qubit\n",
    "parameterized quantum circuit to construct a partial Fourier series for\n",
    "fitting the target function.\n",
    "\n",
    "The main outline of the process is as follows:\n",
    "\n",
    "1.  Build a circuit consisting of layers of alternating data-encoding\n",
    "    and parameterized training blocks.\n",
    "2.  Optimize the expectation value of the circuit output against a\n",
    "    target function to be fitted.\n",
    "3.  Obtain a partial Fourier series for the target function. Since the\n",
    "    function is not periodic, this partial Fourier series will only\n",
    "    approximate the function in the region we will use for training.\n",
    "4.  Plot the optimized circuit expectation value against the exact\n",
    "    function to compare the two.\n",
    "\n",
    "# What is a quantum model?\n",
    "\n",
    "A quantum model $g_{\\vec{\\theta}}(\\vec{x})$ is the expectation value of\n",
    "some observable $M$ estimated on the state prepared by a parameterized\n",
    "circuit $U(\\vec{x}, \\vec{\\theta})$:\n",
    "\n",
    "$$g_{\\vec{\\theta}}(\\vec{x}) = \\langle 0 | U^\\dagger (\\vec{x}, \\vec{\\theta}) M U(\\vec{x}, \\vec{\\theta}) | 0 \\rangle.$$\n",
    "\n",
    "By repeatedly running the circuit with a set of parameters\n",
    "$\\vec{\\theta}$ and set of data points $\\vec{x}$, we can approximate the\n",
    "expectation value of the observable $M$ in the state\n",
    "$U(\\vec{x}, \\vec{\\theta}) | 0 \\rangle.$ Then, the parameters can be\n",
    "optimized to minimize some loss function.\n",
    "\n",
    "# Building the variational circuit\n",
    "\n",
    "In this example, we will use a variational quantum circuit to find the\n",
    "Fourier series that approximates the function\n",
    "$f(x_1, x_2) = \\frac{1}{2} \\left( x_1^2 + x_2^2 \\right)$. The\n",
    "variational circuit that we are using is made up of $L$ layers. Each\n",
    "layer consists of a *data-encoding block* $S(\\vec{x})$ and a *training\n",
    "block* $W(\\vec{\\theta})$. The overall circuit is:\n",
    "\n",
    "$$U(\\vec{x}, \\vec{\\theta}) = W^{(L+1)}(\\vec{\\theta}) S(\\vec{x}) W^{(L)} (\\vec{\\theta}) \\ldots W^{(2)}(\\vec{\\theta}) S(\\vec{x}) W^{(1)}(\\vec{\\theta}).$$\n",
    "\n",
    "The training blocks $W(\\vec{\\theta})$ depend on the parameters\n",
    "$\\vec{\\theta}$ that can be optimized classically.\n",
    "\n",
    "![](../_static/demonstration_assets/qnn_multivariate_regression/qnn_circuit.png){.align-center\n",
    "width=\"90.0%\"}\n",
    "\n",
    "We will build a circuit such that the expectation value of the\n",
    "$Z\\otimes Z$ observable is a partial Fourier series that approximates\n",
    "$f(\\vec{x})$, i.e.,\n",
    "\n",
    "$$g_{\\vec{\\theta}}(\\vec{x})= \\sum_{\\vec{\\omega} \\in \\Omega} c_\\vec{\\omega} e^{i \\vec{\\omega} \\vec{x}} \\approx f(\\vec{x}).$$\n",
    "\n",
    "Then, we can directly plot the partial Fourier series. We can also apply\n",
    "a Fourier transform to $g_{\\vec{\\theta}}$, so we can obtain the Fourier\n",
    "coefficients, $c_\\vec{\\omega}$. To know more about how to obtain the\n",
    "Fourier series, check out these two related tutorials,.\n",
    "\n",
    "# Constructing the quantum circuit\n",
    "\n",
    "First, let\\'s import the necessary libraries and seed the random number\n",
    "generator. We will use Matplotlib for plotting and JAX for optimization.\n",
    "We will also define the device, which has two qubits, using\n",
    "`~.pennylane.device`{.interpreted-text role=\"func\"}.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as pnp\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "import optax\n",
    "\n",
    "pnp.random.seed(42)\n",
    "\n",
    "dev = qml.device('default.qubit', wires=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will construct the data-encoding circuit block, $S(\\vec{x})$, as\n",
    "a product of $R_z$ rotations:\n",
    "\n",
    "$$S(\\vec{x}) = R_z(x_1) \\otimes R_z(x_2).$$\n",
    "\n",
    "Specifically, we define the $S(\\vec{x})$ operator using the\n",
    "`~.pennylane.AngleEmbedding`{.interpreted-text role=\"class\"} function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def S(x):\n",
    "    qml.AngleEmbedding( x, wires=[0,1],rotation='Z')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the $W(\\vec{\\theta})$ operator, we will use an ansatz that is\n",
    "available in PennyLane, called\n",
    "`~.pennylane.StronglyEntanglingLayers`{.interpreted-text role=\"class\"}.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def W(params):\n",
    "    qml.StronglyEntanglingLayers(params, wires=[0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will build the circuit in PennyLane by alternating layers of\n",
    "$W(\\vec{\\theta})$ and $S(\\vec{x})$ layers. On this prepared state, we\n",
    "estimate the expectation value of the $Z\\otimes Z$ operator, using\n",
    "PennyLane\\'s `~.pennylane.expval`{.interpreted-text role=\"func\"}\n",
    "function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "@qml.qnode(dev,interface=\"jax\")\n",
    "def quantum_neural_network(params, x):\n",
    "    layers=len(params[:,0,0])-1\n",
    "    n_wires=len(params[0,:,0])\n",
    "    n_params_rot=len(params[0,0,:])\n",
    "    for i in range(layers):\n",
    "      W(params[i,:,:].reshape(1,n_wires,n_params_rot))\n",
    "      S(x)\n",
    "    W(params[-1,:,:].reshape(1,n_wires,n_params_rot))\n",
    "\n",
    "    return qml.expval(qml.PauliZ(wires=0)@qml.PauliZ(wires=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function we will be fitting is\n",
    "$f(x_1, x_2) = \\frac{1}{2} \\left( x_1^2 + x_2^2 \\right)$, which we will\n",
    "define as `target_function`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def target_function(x):\n",
    "    f=1/2*(x[0]**2+x[1]**2)\n",
    "    return f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will specify the range of $x_1$ and $x_2$ values and store those\n",
    "values in an input data vector. We are fitting the function for\n",
    "$x_1, x_2 \\in [-1, 1]$ using 30 evenly spaced samples for each variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "x1_min=-1\n",
    "x1_max=1\n",
    "x2_min=-1\n",
    "x2_max=1\n",
    "num_samples=30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we build the training data with the exact target function\n",
    "$f(x_1, x_2)$. To do so, it is convenient to create a two-dimensional\n",
    "grid to make sure that, for each value of $x_1,$ we perform a sweep over\n",
    "all the values of $x_2$ and viceversa.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "x1_train=pnp.linspace(x1_min,x1_max, num_samples)\n",
    "x2_train=pnp.linspace(x2_min,x2_max, num_samples)\n",
    "x1_mesh,x2_mesh=pnp.meshgrid(x1_train, x2_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define `x_train` and `y_train` using the above vectors, reshaping\n",
    "them for our convenience\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train:\n",
      " [[-1.         -1.        ]\n",
      " [-0.93103448 -1.        ]\n",
      " [-0.86206897 -1.        ]\n",
      " [-0.79310345 -1.        ]\n",
      " [-0.72413793 -1.        ]]\n",
      "y_train:\n",
      " [[1.        ]\n",
      " [0.9334126 ]\n",
      " [0.87158145]\n",
      " [0.81450654]\n",
      " [0.76218787]]\n"
     ]
    }
   ],
   "source": [
    "x_train=pnp.stack((x1_mesh.flatten(), x2_mesh.flatten()), axis=1)\n",
    "y_train = target_function([x1_mesh,x2_mesh]).reshape(-1,1)\n",
    "# Let's take a look at how they look like\n",
    "print(\"x_train:\\n\", x_train[:5])\n",
    "print(\"y_train:\\n\", y_train[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing the circuit\n",
    "\n",
    "We want to optimize the circuit above so that the expectation value of\n",
    "$Z \\otimes Z$ approximates the exact target function. This is done by\n",
    "minimizing the mean squared error between the circuit output and the\n",
    "exact target function. In particular, the optimization process to train\n",
    "the variational circuit will be performed using JAX, an auto\n",
    "differentiable machine learning framework to accelerate the classical\n",
    "optimization of the parameters. Check out to learn more about how to use\n",
    "JAX to optimize your QML models.\n",
    "\n",
    "![](../_static/demonstration_assets/qnn_multivariate_regression/qnn_diagram.jpg){.align-center\n",
    "width=\"90.0%\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def mse(params,x,targets):\n",
    "    # We compute the mean square error between the target function and the quantum circuit to quantify the quality of our estimator\n",
    "    return (quantum_neural_network(params,x)-jnp.array(targets))**2\n",
    "@jax.jit\n",
    "def loss_fn(params, x,targets):\n",
    "    # We define the loss function to feed our optimizer\n",
    "    mse_pred = jax.vmap(mse,in_axes=(None, 0,0))(params,x,targets)\n",
    "    loss = jnp.mean(mse_pred)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we are choosing an Adam optimizer with a learning rate of 0.05 and\n",
    "300 steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "opt = optax.adam(learning_rate=0.05)\n",
    "max_steps=300\n",
    "\n",
    "@jax.jit\n",
    "def update_step_jit(i, args):\n",
    "    # We loop over this function to optimize the trainable parameters\n",
    "    params, opt_state, data, targets, print_training = args\n",
    "    loss_val, grads = jax.value_and_grad(loss_fn)(params, data, targets)\n",
    "    updates, opt_state = opt.update(grads, opt_state)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "\n",
    "    def print_fn():\n",
    "        jax.debug.print(\"Step: {i}  Loss: {loss_val}\", i=i, loss_val=loss_val)\n",
    "    # if print_training=True, print the loss every 50 steps\n",
    "    jax.lax.cond((jnp.mod(i, 50) == 0 ) & print_training, print_fn, lambda: None)\n",
    "    return (params, opt_state, data, targets, print_training)\n",
    "\n",
    "@jax.jit\n",
    "def optimization_jit(params, data, targets, print_training=False):\n",
    "    opt_state = opt.init(params)\n",
    "    args = (params, opt_state, jnp.asarray(data), targets, print_training)\n",
    "    # We loop over update_step_jit max_steps iterations to optimize the parameters\n",
    "    (params, opt_state, _, _, _) = jax.lax.fori_loop(0, max_steps+1, update_step_jit, args)\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will train the variational circuit with 4 layers and obtain a\n",
    "vector $\\vec{\\theta}$ with the optimized parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0  Loss: 0.16029931604862213\n",
      "Step: 50  Loss: 0.025749022141098976\n",
      "Step: 100  Loss: 0.002402243670076132\n",
      "Step: 150  Loss: 0.0014659196604043245\n",
      "Step: 200  Loss: 0.0013561941450461745\n",
      "Step: 250  Loss: 0.0013045483501628041\n",
      "Step: 300  Loss: 0.0012705246917903423\n"
     ]
    }
   ],
   "source": [
    "wires=2\n",
    "layers=4\n",
    "params_shape = qml.StronglyEntanglingLayers.shape(n_layers=layers+1,n_wires=wires)\n",
    "params=pnp.random.default_rng().random(size=params_shape)\n",
    "best_params=optimization_jit(params, x_train, jnp.array(y_train), print_training=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run this yourself, you\\'ll see that the training step with JAX is\n",
    "extremely fast! Once the optimized $\\vec{\\theta}$ has been obtained, we\n",
    "can use those parameters to build our fitted version of the function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'jax.core' has no attribute 'ConcreteArray'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m     y_pred = jax.vmap(quantum_neural_network, in_axes=(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[32m0\u001b[39m))(params, data)\n\u001b[32m      3\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m y_pred\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m y_predictions=\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mevaluate\u001b[39m\u001b[34m(params, data)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mevaluate\u001b[39m(params, data):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     y_pred = \u001b[43mjax\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquantum_neural_network\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_axes\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m y_pred\n",
      "    \u001b[31m[... skipping hidden 7 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/qml/lib/python3.12/site-packages/pennylane/workflow/qnode.py:905\u001b[39m, in \u001b[36mQNode.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    903\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m qml.capture.enabled():\n\u001b[32m    904\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m capture_qnode(\u001b[38;5;28mself\u001b[39m, *args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m905\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_impl_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/qml/lib/python3.12/site-packages/pennylane/workflow/qnode.py:881\u001b[39m, in \u001b[36mQNode._impl_call\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    878\u001b[39m \u001b[38;5;66;03m# Calculate the classical jacobians if necessary\u001b[39;00m\n\u001b[32m    879\u001b[39m \u001b[38;5;28mself\u001b[39m._transform_program.set_classical_component(\u001b[38;5;28mself\u001b[39m, args, kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m881\u001b[39m res = \u001b[43mqml\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    882\u001b[39m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdiff_method\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdiff_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    885\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterface\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterface\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtransform_program\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_transform_program\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    887\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgradient_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgradient_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    888\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexecute_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    890\u001b[39m res = res[\u001b[32m0\u001b[39m]\n\u001b[32m    892\u001b[39m \u001b[38;5;66;03m# convert result to the interface in case the qfunc has no parameters\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/qml/lib/python3.12/site-packages/pennylane/workflow/execution.py:195\u001b[39m, in \u001b[36mexecute\u001b[39m\u001b[34m(tapes, device, diff_method, interface, transform_program, inner_transform, config, grad_on_execution, gradient_kwargs, cache, cachesize, max_diff, device_vjp, mcm_config, gradient_fn)\u001b[39m\n\u001b[32m    191\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ()\n\u001b[32m    193\u001b[39m \u001b[38;5;66;03m### Specifying and preprocessing variables ####\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m195\u001b[39m interface = \u001b[43m_resolve_interface\u001b[49m\u001b[43m(\u001b[49m\u001b[43minterface\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[38;5;66;03m# Only need to calculate derivatives with jax when we know it will be executed later.\u001b[39;00m\n\u001b[32m    198\u001b[39m gradient_kwargs = gradient_kwargs \u001b[38;5;129;01mor\u001b[39;00m {}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/qml/lib/python3.12/site-packages/pennylane/workflow/resolution.py:124\u001b[39m, in \u001b[36m_resolve_interface\u001b[39m\u001b[34m(interface, tapes)\u001b[39m\n\u001b[32m    118\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[32m    119\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m qml.QuantumFunctionError(  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[32m    120\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mjax not found. Please install the latest \u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[32m    121\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mversion of jax to enable the \u001b[39m\u001b[33m'\u001b[39m\u001b[33mjax\u001b[39m\u001b[33m'\u001b[39m\u001b[33m interface.\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[32m    122\u001b[39m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m     interface = \u001b[43m_get_jax_interface_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m interface\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/qml/lib/python3.12/site-packages/pennylane/workflow/resolution.py:68\u001b[39m, in \u001b[36m_get_jax_interface_name\u001b[39m\u001b[34m(tapes)\u001b[39m\n\u001b[32m     65\u001b[39m             op = \u001b[38;5;28mgetattr\u001b[39m(op, \u001b[33m\"\u001b[39m\u001b[33mobs\u001b[39m\u001b[33m\"\u001b[39m, op)\n\u001b[32m     66\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m op \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     67\u001b[39m             \u001b[38;5;66;03m# Some MeasurementProcess objects have op.obs=None\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43many\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mqml\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_abstract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mparam\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m     69\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m Interface.JAX_JIT\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Interface.JAX\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/qml/lib/python3.12/site-packages/pennylane/workflow/resolution.py:68\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     65\u001b[39m             op = \u001b[38;5;28mgetattr\u001b[39m(op, \u001b[33m\"\u001b[39m\u001b[33mobs\u001b[39m\u001b[33m\"\u001b[39m, op)\n\u001b[32m     66\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m op \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     67\u001b[39m             \u001b[38;5;66;03m# Some MeasurementProcess objects have op.obs=None\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[43mqml\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_abstract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m op.data):\n\u001b[32m     69\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m Interface.JAX_JIT\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Interface.JAX\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/qml/lib/python3.12/site-packages/pennylane/math/utils.py:282\u001b[39m, in \u001b[36mis_abstract\u001b[39m\u001b[34m(tensor, like)\u001b[39m\n\u001b[32m    269\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjax\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minterpreters\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpartial_eval\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DynamicJaxprTracer\n\u001b[32m    271\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    272\u001b[39m         tensor,\n\u001b[32m    273\u001b[39m         (\n\u001b[32m   (...)\u001b[39m\u001b[32m    280\u001b[39m         \u001b[38;5;66;03m# If the value of the tracer is known, it will contain a ConcreteArray.\u001b[39;00m\n\u001b[32m    281\u001b[39m         \u001b[38;5;66;03m# Otherwise, it will be abstract.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensor.aval, \u001b[43mjax\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcore\u001b[49m\u001b[43m.\u001b[49m\u001b[43mConcreteArray\u001b[49m)\n\u001b[32m    284\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensor, DynamicJaxprTracer)\n\u001b[32m    286\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m interface == \u001b[33m\"\u001b[39m\u001b[33mtensorflow\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/qml/lib/python3.12/site-packages/jax/_src/deprecations.py:57\u001b[39m, in \u001b[36mdeprecation_getattr.<locals>.getattr\u001b[39m\u001b[34m(name)\u001b[39m\n\u001b[32m     55\u001b[39m   warnings.warn(message, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m fn\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmodule \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: module 'jax.core' has no attribute 'ConcreteArray'"
     ]
    }
   ],
   "source": [
    "def evaluate(params, data):\n",
    "    y_pred = jax.vmap(quantum_neural_network, in_axes=(None, 0))(params, data)\n",
    "    return y_pred\n",
    "y_predictions=evaluate(best_params,x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare the fitted function to the exact target function, let\\'s take\n",
    "a look at the $R^2$ score:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2 = round(float(r2_score(y_train, y_predictions)),3)\n",
    "print(\"R^2 Score:\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let\\'s now plot the results to visually check how good our fit is!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "# Target function\n",
    "ax1 = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "ax1.plot_surface(x1_mesh,x2_mesh, y_train.reshape(x1_mesh.shape), cmap='viridis')\n",
    "ax1.set_zlim(0,1)\n",
    "ax1.set_xlabel('$x$',fontsize=10)\n",
    "ax1.set_ylabel('$y$',fontsize=10)\n",
    "ax1.set_zlabel('$f(x,y)$',fontsize=10)\n",
    "ax1.set_title('Target ')\n",
    "\n",
    "# Predictions\n",
    "ax2 = fig.add_subplot(1, 2, 2, projection='3d')\n",
    "ax2.plot_surface(x1_mesh,x2_mesh, y_predictions.reshape(x1_mesh.shape), cmap='viridis')\n",
    "ax2.set_zlim(0,1)\n",
    "ax2.set_xlabel('$x$',fontsize=10)\n",
    "ax2.set_ylabel('$y$',fontsize=10)\n",
    "ax2.set_zlabel('$f(x,y)$',fontsize=10)\n",
    "ax2.set_title(f' Predicted \\nAccuracy: {round(r2*100,3)}%')\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout(pad=3.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! We have managed to successfully fit a multidimensional function\n",
    "using a parametrized quantum circuit!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "In this demo, we\\'ve shown how to utilize a variational quantum circuit\n",
    "to solve a regression problem for a two-dimensional function. The\n",
    "results show a good agreement with the target function and the model can\n",
    "be trained further, increasing the number of iterations in the training\n",
    "to maximize the accuracy. It also paves the way for addressing a\n",
    "regression problem for an $N$-dimensional function, as everything\n",
    "presented here can be easily generalized. A final check that could be\n",
    "done is to obtain the Fourier coefficients of the trained circuit and\n",
    "compare it with the Fourier series we obtained directly from the target\n",
    "function.\n",
    "\n",
    "# References\n",
    "\n",
    "# About the authors\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
